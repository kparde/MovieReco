{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set Creation \n",
    "\n",
    "__Raw Data__: \n",
    "- MovieLens\n",
    "    - Movies: movies with ID numbers, title, release year, genre\n",
    "    - Ratings: user Ids with ratings for movieIds. 0.5-5 starts on 0.5 increments\n",
    "    - Tags: all possible genome tags \n",
    "    - Relevance: list of all genome tags for each movie with relevance score (0-100)\n",
    "    - Links: movieId with corresponding imdbId\n",
    "- IMDB: movie, imdbId, actors, directors, description, genres, production company, year of release, country. Plus additional columns not using for content vectors: duration, language, income, budget, user reviews, critics reviews\n",
    "    - Limited amount of missing data for director, actors, country, description, production company. Small numbers. Plus can still use the non-missing data for a given movie to build its content vector/profile\n",
    "\n",
    "__Data Creation__: \n",
    "- Merge in IMDB data to get additional metadata \n",
    "    - First merge movies + links to get imdbId \n",
    "    - Lose 17140 movies that aren't in IMDB dataset. Could not find larger dataset that had imdbIDs for merging\n",
    "- Limit ratings data to movies in data post imdb merge\n",
    "    - Do not have to limit overall movies to ones with ratings. Can recommend movies without ratings based on their content profile\n",
    "    - Save version of ratings data with 25% random user subsample to use in UI to improve processing time\n",
    "- Calculate aggregate rating data by movie for use in evaluations, sorting recommendations etc. Save as parquet. \n",
    "    - Average rating\n",
    "    - Count of ratings\n",
    "- Define decade of release\n",
    "- Merge genome tags + relevance scores for each movie\n",
    "    - 28% of movies have tags\n",
    "- Clean genre: IMDB and MovieLens have slightly different lists. Take union of two. \n",
    "- Text cleaning for description and genome tags\n",
    "- Find top 5 relevant genome tags for each movie \n",
    "- Find top 5 TFIDF tokens for each movie for following fields:\n",
    "    - Description\n",
    "    - Genome Tags (relevance > 75%)\n",
    "    - Text: Tags + Description combined into one field \n",
    "- Separate delimited fields into lists (ex multiple actors per movie in format actor1|actor2)\n",
    "- For each metadata, find counts (# of movies) per value and exclude values that are only in 1 movie --> not useful for content profile comparison and decrease memory\n",
    "- One hot encode content features\n",
    "    - For actors, too many values to fit all encodings in memory. Get top 3 actors in each movie based on number of other movies they have been in. Then check again and exclude actors only in 1 movies out of this subset\n",
    "    - Create multiple versions of data with different combinations of feature vectors4\n",
    "    \n",
    "    \n",
    "__Outputted Datasets__: \n",
    "- ratings_sample.parq: sampled ratings dataset + nonIMDB movies excluded \n",
    "- movies_ratings.parq: average and total reviews for each movie\n",
    "- movies_processed.parq: processed movies dataframe before one hot encoding and exclusions necessary for that encoding. Used for data display in UI (additional featuers added in recommendation_data_display) and for EDA (movies_EDA)\n",
    "- processed_df + <>.parq ; processed_df + <>.npz ; sparse_metadata + <>\n",
    "    - Variety of datasets with different one hot encoded features for building content model \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as datetime\n",
    "import operator\n",
    "import fastparquet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import scipy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import re\n",
    "#nlp = spacy.load('en')\n",
    "#nltk.download('stopwords')\n",
    "sw = stopwords.words(\"english\")\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links = pd.read_csv('data/ml-25m/links.csv')\n",
    "movies = pd.read_csv('data/ml-25m/movies.csv')\n",
    "ratings = pd.read_csv('data/ml-25m/ratings.csv')\n",
    "tags = pd.read_csv('data/ml-25m/genome-tags.csv')\n",
    "relevance = pd.read_csv('data/ml-25m/genome-scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all Data (including IMDB dataset)\n",
    "- Movies + Links. On movieId \n",
    "- Merge in IMDB. On imdbId from links \n",
    "\n",
    "Searched kaggle for a larger set and did not find one that also had IMDB IDs    \n",
    "Drop movies that are not in IMDB because need metadata to recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB dataset\n",
    "imdb_movies = pd.read_csv('data/imdb/IMDb movies.csv')\n",
    "\n",
    "# standardize IMDB IDs\n",
    "imdb_movies['imdbId'] = imdb_movies.imdb_title_id.str.split('tt').str[1]\n",
    "imdb_movies.imdbId = pd.to_numeric(imdb_movies.imdbId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_movies = len(movies)\n",
    "# links (has imdb rating) + movies\n",
    "df = pd.merge(links, movies, on = 'movieId')\n",
    "# merge with imdb data\n",
    "# INNER merge so that only get movies that are in movielens data + have content from IMDB \n",
    "df = pd.merge(df, imdb_movies, on = 'imdbId', how = 'inner')\n",
    "\n",
    "# titles are different in movielens vs imdb because imdb in the original language whereas movielens all english translated\n",
    "df = df.rename(columns = {'title_x':'title_eng', 'title_y':'title_orig'})\n",
    "\n",
    "new_num_movies = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit to relevant columns\n",
    "df = df[['movieId', 'title_eng', 'year', 'genre', 'genres', 'director', 'actors', 'country', \n",
    "         'description', 'production_company']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data Post Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17410"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of movies lost in merge\n",
    "num_movies - new_num_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45013"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_num_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movieId                  0\n",
       "title_eng                0\n",
       "year                     0\n",
       "genre                    0\n",
       "genres                   0\n",
       "director                13\n",
       "actors                  20\n",
       "country                 10\n",
       "description            404\n",
       "production_company    1230\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit Ratings Data to Movies in IMDB + Save Ratings Data\n",
    "To build user profiles based on ratings, need movie content info    \n",
    "Do NOT however have to limit movie data to movies in ratings. Can recommend movies with no ratings.    \n",
    "  \n",
    "- ratings_sample: ratings data for evaluation    \n",
    "- ratings_sample_user add: random 25% sample of users for loading into UI app to increase performance. Most people will manually enter profiles, but want some preloaded to show that functionality as well. This will also be the dataset that we add the entered profiles onto.\n",
    "    - Subsample based on users rather than observations so that each user has their full set of ratings to get recommendations based on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings data for evaluation\n",
    "ratings = ratings[ratings.movieId.isin(df.movieId.unique())]\n",
    "ratings.to_parquet('processed_files/ratings_sample.parq', engine = 'fastparquet', compression = 'GZIP', index = False)\n",
    "\n",
    "# ratings data for app\n",
    "random.seed(1)\n",
    "random_users = random.sample(list(ratings.userId.unique()), int(len(ratings.userId.unique())*.25))\n",
    "ratings_useradd = ratings[ratings.userId.isin(random_users)]\n",
    "ratings_useradd = ratings_useradd.drop(columns = ['timestamp'])\n",
    "ratings_useradd.to_parquet('processed_files/ratings_sample_useradd.parq', engine = 'fastparquet', compression = 'GZIP', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate count and average of ratings by movie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_ratings_agg(df_full, var):\n",
    "    \n",
    "    ratings_gb = df_full.groupby(var).rating.mean().to_frame()\n",
    "    ratings_gb.columns = ['avg']\n",
    "    ratings_gb['id'] = ratings_gb.index\n",
    "    ratings_gb = pd.merge(ratings_gb, df[['movieId', 'title_eng']], left_on = 'id', right_on = 'movieId')\n",
    "\n",
    "    # many movies with average = 5, but many of these have only been rated a few times\n",
    "    # merge and secondary sort by number of ratings\n",
    "    num_ratings = df_full.groupby('movieId').rating.count().to_frame()\n",
    "    num_ratings.columns = ['cnt']\n",
    "    num_ratings['id'] = num_ratings.index\n",
    "\n",
    "    ratings_gb = pd.merge(ratings_gb, num_ratings, on = 'id')\n",
    "\n",
    "    # sort on rating and number of ratings\n",
    "    ratings_gb = ratings_gb.sort_values(['avg',  'cnt'], ascending = True)\n",
    "    \n",
    "    return ratings_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Most highly rated movies: Average with at least 100 ratings__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge rating dataset with movies\n",
    "# lose some movies in merge because not in imdb movie set \n",
    "df_full = pd.merge(df, ratings, on = 'movieId')\n",
    "\n",
    "# get aggregated movie ratings\n",
    "movie_ratings = movie_ratings_agg(df_full, 'movieId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Save df for evaluations, sorting of other recommendations etc._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ratings.to_parquet('processed_files/movies_ratings.parq', engine = 'fastparquet', compression = 'GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Release Decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# decade to filter by \n",
    "def rounddown(row):\n",
    "    return int(math.floor(row / 10.0)) * 10\n",
    "\n",
    "df['decade'] = df.year.apply(lambda row: rounddown(row))\n",
    "# convert to string for filtering with lists\n",
    "df.decade = df.decade.apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genome Tagging Data: Merge Tags + Relevance \n",
    "Two perspectives for metadata (created later):\n",
    "- Top 5 tags per movie in terms of relevance score\n",
    "- Top 5 tags per movie in terms of TF-IDF score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge tags and relevance scores\n",
    "tags = pd.merge(tags, relevance, on = 'tagId')\n",
    "\n",
    "# exclude movies not in main df\n",
    "tags = tags[tags.movieId.isin(df.movieId.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2766978428454002"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percent of movies with tags\n",
    "tags.movieId.nunique() / df.movieId.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Genre Variable\n",
    "- IMDB and MovieLens sometimes have a different genre list for the same movie. Take the union of both lists to get the max number of genres/information.  \n",
    "- MovieLens: genres\n",
    "- IMDB: genre\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movielens missing value\n",
    "df.genres = np.where(df.genres == '(no genres listed)', np.nan, df.genres)\n",
    "\n",
    "# convert into sets and take union \n",
    "df.genre = df.genre.str.split(', ')\n",
    "df.genres = df.genres.str.split('|')\n",
    "\n",
    "# when missing in movielens, replace to list rather than nan \n",
    "df.genres = df.genres.apply(lambda d: d if isinstance(d, list) else [])\n",
    "\n",
    "df.genre = df.genre.apply(set)\n",
    "df.genres = df.genres.apply(set)\n",
    "\n",
    "df['genres_all'] = df.apply(lambda x: x['genre'].union(x['genres']), axis=1)\n",
    "df.genres_all = df.genres_all.apply(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Delimiters into Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for var in ['director', 'actors', 'country']:\n",
    "    df[var + '_lst'] = df[var].str.split(', ')\n",
    "    df[var + '_lst'] = df[var + '_lst'].apply(lambda d: d if isinstance(d, list) else [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning: Description, Genome Tags\n",
    "- Downcase, remove non-ASCII, remove punctuation, trim white space\n",
    "- Tokenize\n",
    "- Remove stop words (except no and not - commonly  used as negation in tags)\n",
    "- Lemmatize\n",
    "- Construct token lists into \"sentences\" separated by spaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear up memory \n",
    "del imdb_movies\n",
    "del movies\n",
    "del links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords: exclude no and not -- used in tags as negations (ex not funny vs funny ; no plot vs plot)\n",
    "sw = [w for w in sw if w not in ['not', 'no']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prep(df, var, clean = True):\n",
    "    \n",
    "    ## clean text\n",
    "    \n",
    "    if clean:\n",
    "    \n",
    "        # replace missing with empty string\n",
    "        df[var] = np.where(df[var].isnull(), '', df[var])\n",
    "\n",
    "        # lower case\n",
    "        df[var] = df[var].str.lower()\n",
    "        # remove non-ascii\n",
    "        df[var] = df[var].apply(lambda x: ''.join([\" \" if ord(i) < 32 or ord(i) > 126 else i for i in str(x)])) \n",
    "        # replace dash with space\n",
    "        df[var] = df[var].apply(lambda x: re.sub('-', ' ', x))\n",
    "        # remove punctuation. Replace with '' so keep contractions together\n",
    "        df[var] = df[var].apply(lambda x: re.sub(r'[^\\w\\s]','', x))\n",
    "        # remove double and triple spaces\n",
    "        df[var] = df[var].apply(lambda x: re.sub(' +', ' ', x)) \n",
    "        # remove white space trailing/leading\n",
    "        df[var] = df[var].apply(lambda x: str(x).strip()) \n",
    "        \n",
    "        # tokenize\n",
    "        df[var] = df[var].apply(lambda row: row.split(' '))\n",
    "    \n",
    "    ## text processing\n",
    "\n",
    "    # remove stop words\n",
    "    df[var] = df[var].apply(lambda row: [w for w in row if w not in sw])\n",
    "\n",
    "    # lemmatize. Do NOT remove proper nouns/do POS tagging\n",
    "    df[var] = df[var].apply(lemmatize_text)\n",
    "\n",
    "    # reconstruct sentences\n",
    "    df[var] = df[var].apply(lambda row: ' '.join(row))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 5 Relevant Tags for each movie \n",
    "- Clean text for tags \n",
    "- Group by cleaned version and take maximum relevance score. Thus dropping \"duplicate\" tags like zombie and zombies\n",
    "    - Manually define group of duplicates around based on a book\n",
    "- For each movie, get the top 5 relevant cleaned tags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Drop \"Duplicate\" Tags__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clean list of tags to find duplicate tags (ex zombie, zombies ; based on a book, based on book). Keep original version\n",
    "unique_tags = tags[['tag']].drop_duplicates()\n",
    "unique_tags['tag_cleaned'] = unique_tags.tag\n",
    "unique_tags = text_prep(unique_tags, 'tag_cleaned', clean = True)\n",
    "\n",
    "# manual cleaning: very common tag with several versions \n",
    "unique_tags.tag_cleaned = np.where(unique_tags.tag_cleaned.isin(['literary adaptation', 'adapted from book',\n",
    "                                                                 'adaptation', 'based book', 'book', 'adapted frombook']),\n",
    "                                   'adapted from book', unique_tags.tag_cleaned)\n",
    "\n",
    "# Merge with original tag data. For each movie, keep duplicate token with higher relevance score \n",
    "tags = pd.merge(tags, unique_tags, on = 'tag')\n",
    "tags = tags.groupby(['movieId', 'tag_cleaned']).relevance.max()\n",
    "tags = tags.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Top 5 Relevant Genome Tags__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# top 5 relevant tags for each movie\n",
    "tags_top5 = tags.sort_values(['movieId', 'relevance'], ascending = False)\n",
    "tags_top5 = tags_top5.groupby('movieId').head(5)\n",
    "tags_top5 = tags_top5.groupby('movieId').tag_cleaned.unique().to_frame()\n",
    "tags_top5.columns = ['tags_rel']\n",
    "\n",
    "# merge with dataframe\n",
    "df = pd.merge(df, tags_top5, left_on = 'movieId', right_index = True, how = 'left')\n",
    "\n",
    "# combine multiple word tags into a single token \n",
    "df.tags_rel = np.where(df.tags_rel.isnull(), '', df.tags_rel)\n",
    "df.tags_rel = df.tags_rel.apply(lambda row: [i.replace(' ', '') for i in row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 5 TFIDF: Description, Tags, Text (Tags + Description) \n",
    "- TF-IDF vectors\n",
    "- For each movie, get top 5 tfidf vectors \n",
    "- Save to pickle (long process especially if description involved)    \n",
    "    \n",
    "For tags, only consider for tfidf if above 75% relevance score for a movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(df, var, filename):\n",
    "    \n",
    "    ## TF-IDF\n",
    "    tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 1),min_df=0, stop_words='english')\n",
    "    tfidf_matrix = tf.fit_transform(df[var])\n",
    "\n",
    "    # For each movie, list of columns with a term in tf-idf matrix \n",
    "    nonzero_cols = np.split(tfidf_matrix.indices, tfidf_matrix.indptr)[1:-1]\n",
    "    \n",
    "    # get top 5 tf-idf tokens for each movie. output is column number in tfidf\n",
    "    top5_cols_lst = []\n",
    "    for i in range(tfidf_matrix.shape[0]):\n",
    "        top5_cols = np.argsort(tfidf_matrix[0,list(nonzero_cols[i])].toarray()[0])[::-1][:5]\n",
    "        top5_cols = nonzero_cols[i][top5_cols]\n",
    "        top5_cols_lst.append(top5_cols)\n",
    "\n",
    "    # get words for those column numbers\n",
    "    top5_cols_lst_names = [[tf.get_feature_names()[i] for i in j] for j in top5_cols_lst]\n",
    "\n",
    "    with open(filename + '.pkl', 'wb') as f:\n",
    "        pickle.dump(top5_cols_lst_names, f)\n",
    "        \n",
    "    return top5_cols_lst_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Top 5 TFIDF Description__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean description text\n",
    "df = text_prep(df, 'description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#top5_cols_lst_names = tfidf(df, 'description', \"processed_files/tfidf_top5_cols\") -- 1.5 hours. Load from pickle instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_files/tfidf_top5_cols.pkl', 'rb') as f:\n",
    "    top5_cols_lst_names = pickle.load(f)\n",
    "    \n",
    "top5_cols_lst_names = pd.Series(top5_cols_lst_names).to_frame()\n",
    "top5_cols_lst_names.columns = ['desc_top5']\n",
    "\n",
    "# merge top5 tokens with df\n",
    "df = pd.merge(df, top5_cols_lst_names, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Top 5 TFIDF Genome Tags__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# limit to relevance >= 75% and merge in \n",
    "tags_movies = tags[tags.relevance > 0.75].groupby('movieId').tag_cleaned.unique()\n",
    "df = pd.merge(df, tags_movies, on = 'movieId', how = 'left')\n",
    "\n",
    "# combine multiple word tags into a single token \n",
    "df.tag_cleaned = np.where(df.tag_cleaned.isnull(), '', df.tag_cleaned)\n",
    "df.tag_cleaned = df.tag_cleaned.apply(lambda row: [i.replace(' ', '') for i in row])\n",
    "\n",
    "# combine into sentences for tfidf\n",
    "df.tag_cleaned = df.tag_cleaned.apply(lambda row: ' '.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top5_cols_lst_names_tags = tfidf(df, 'tag_cleaned', \"processed_files/tfidf_top5_tags_cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_files/tfidf_top5_tags_cols.pkl', 'rb') as f:\n",
    "    top5_cols_lst_names_tags = pickle.load(f)\n",
    "    \n",
    "top5_cols_lst_names_tags = pd.Series(top5_cols_lst_names_tags).to_frame()\n",
    "top5_cols_lst_names_tags.columns = ['tag_top5']\n",
    "\n",
    "# merge top5 tokens with df\n",
    "df = pd.merge(df, top5_cols_lst_names_tags, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Top 5 TFIDF Description + Tags__    \n",
    "Only ~25% of movies have tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['text'] = df.tag_cleaned + df.description\n",
    "df.text = np.where(df.text.isnull(), '', df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#top5_cols_lst_names_text = tfidf(df, 'text', \"tfidf_top5_text_cols\")  # 2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'processed_files/tfidf_top5_text_cols.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-1a0fec05c24b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'processed_files/tfidf_top5_text_cols.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtop5_cols_lst_names_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtop5_cols_lst_names_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop5_cols_lst_names_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtop5_cols_lst_names_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'text_top5'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'processed_files/tfidf_top5_text_cols.pkl'"
     ]
    }
   ],
   "source": [
    "with open('processed_files/tfidf_top5_text_cols.pkl', 'rb') as f:\n",
    "    top5_cols_lst_names_text = pickle.load(f)\n",
    "    \n",
    "top5_cols_lst_names_text = pd.Series(top5_cols_lst_names_text).to_frame()\n",
    "top5_cols_lst_names_text.columns = ['text_top5']\n",
    "\n",
    "# merge top5 tokens with df\n",
    "df = pd.merge(df, top5_cols_lst_names_text, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dataframe for Display in UI and EDA\n",
    "Before one hot encoding and filtering out actors etc.   \n",
    "Keep cleaned version of tags for display     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_save = df.copy()\n",
    "df_save['tag'] = df_save.tag_cleaned.apply(lambda row: row.split(' '))\n",
    "df_save = df_save.drop(columns = ['genre', 'genres'])\n",
    "df_save.to_parquet('processed_files/movies_processed.parq', engine = 'fastparquet', compression = 'GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each delimited variable, get dataframe of occurrence counts\n",
    "\n",
    "Ex: Tom Hanks | Leonardo Dicaprio      \n",
    "    Tom Hanks        \n",
    "    Angelina Jolie    \n",
    "    \n",
    "Tom Hanks 2       \n",
    "Leonardo Dicaprio 1       \n",
    "Angeline Jolie 1         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_list_expand(df, var):\n",
    "    \n",
    "    # expand lists such that one entry per row \n",
    "    expanded = df[[var, 'movieId']]\n",
    "    expanded = pd.DataFrame({\n",
    "        col:np.repeat(expanded[col].values, expanded[var].str.len()) for col in expanded.columns.drop(var)}\n",
    "    ).assign(**{var:np.concatenate(expanded[var].values)})[expanded.columns]\n",
    "\n",
    "    return expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delimited_count(df, var, new_var):\n",
    "    # expand lists such that one entry per row \n",
    "    expanded = cat_list_expand(df, var)\n",
    "\n",
    "    # groupby and count # of occurrences\n",
    "    counts = expanded.groupby(var)[var].count().to_frame()\n",
    "    counts.columns = ['cnt']\n",
    "    # sort\n",
    "    counts = counts.sort_values(['cnt'])\n",
    "    counts[new_var] = counts.index\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "genre_counts = delimited_count(df, 'genres_all', 'genres')\n",
    "actors_counts = delimited_count(df, 'actors_lst', 'actors')\n",
    "directors_counts = delimited_count(df, 'director_lst', 'director')\n",
    "country_counts = delimited_count(df, 'country_lst', 'country')\n",
    "desc_counts = delimited_count(df, 'desc_top5', 'desc')\n",
    "tag_counts = delimited_count(df, 'tag_top5', 'tag')\n",
    "tag_rel_counts = delimited_count(df, 'tags_rel', 'tag_rel')\n",
    "text_counts = delimited_count(df, 'text_top5', 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Lists of values to exclude if only in 1 movie  \n",
    "Not helpful for identifying similar movies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Directors__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5920960909432141\n"
     ]
    }
   ],
   "source": [
    "print(len(directors_counts[directors_counts.cnt == 1]) / len(directors_counts))\n",
    "\n",
    "directors_exclude = directors_counts[directors_counts.cnt == 1].director.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Actors__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6667830620373791\n"
     ]
    }
   ],
   "source": [
    "print(len(actors_counts[actors_counts.cnt == 1]) / len(actors_counts))\n",
    "\n",
    "actors_exclude = actors_counts[actors_counts.cnt == 1].actors.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Countries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17142857142857143\n"
     ]
    }
   ],
   "source": [
    "print(len(country_counts[country_counts.cnt == 1]) / len(country_counts))\n",
    "\n",
    "country_exclude = country_counts[country_counts.cnt == 1].country.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Production Companies__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7027213227695488\n"
     ]
    }
   ],
   "source": [
    "prod_counts = df.groupby('production_company').production_company.count().to_frame()\n",
    "prod_counts.columns = ['cnt']\n",
    "# sort\n",
    "prod_counts = prod_counts.sort_values(['cnt'])\n",
    "prod_counts['production_company'] = prod_counts.index\n",
    "    \n",
    "print(len(prod_counts[prod_counts.cnt == 1]) / len(prod_counts))\n",
    "\n",
    "prod_exclude = prod_counts[prod_counts.cnt == 1].index.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Description Tokens__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5089156626506024\n"
     ]
    }
   ],
   "source": [
    "print(len(desc_counts[desc_counts.cnt == 1]) / len(desc_counts))\n",
    "\n",
    "desc_exclude = desc_counts[desc_counts.cnt == 1].desc.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tag Tokens__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04196519959058342\n"
     ]
    }
   ],
   "source": [
    "print(len(tag_counts[tag_counts.cnt == 1]) / len(tag_counts))\n",
    "\n",
    "tags_exclude = tag_counts[tag_counts.cnt == 1].tag.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tag Relevance__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5175674142430317\n"
     ]
    }
   ],
   "source": [
    "print(len(text_counts[text_counts.cnt == 1]) / len(text_counts))\n",
    "\n",
    "text_exclude = text_counts[text_counts.cnt == 1].text.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Text Tokens__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03736479842674533\n"
     ]
    }
   ],
   "source": [
    "print(len(tag_rel_counts[tag_rel_counts.cnt == 1]) / len(tag_rel_counts))\n",
    "\n",
    "tags_rel_exclude = tag_rel_counts[tag_rel_counts.cnt == 1].tag_rel.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encode Content Variables  \n",
    "- Exclude values that are only in 1 movie: not useful for recommendations (+ reduce memory)   \n",
    "\n",
    "Additional cleaning for actors: very large quantity if include all actors in all movies. Too much memory.   \n",
    "- Get top 3 actors from each movies. \"Top\" defined by how many movies they have appeared in \n",
    "- Check again for actors only in 1 movie after top3 exclusion \n",
    "\n",
    "Create one hot encodings for a variety of datasets with different combinations of features\n",
    "   \n",
    "Keep movies that do not have any actors, directors, countries based on the above exclusions. Can still be recommended based on other attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exclude Rare Actors, Directors, Countries: Not Useful for Recommendations__    \n",
    "And including all results in memory errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# exclude if only in 1 movie (in dataset)\n",
    "df.actors_lst = df.actors_lst.map(set) - set(actors_exclude)\n",
    "df.director_lst = df.director_lst.map(set) - set(directors_exclude)\n",
    "df.country_lst = df.country_lst.map(set) - set(country_exclude)\n",
    "df.desc_top5 = df.desc_top5.map(set) - set(desc_exclude)\n",
    "df.tag_top5 = df.tag_top5.map(set) - set(tags_exclude)\n",
    "df.tags_rel = df.tags_rel.map(set) - set(tags_rel_exclude)\n",
    "df.text_top5 = df.text_top5.map(set) - set(text_exclude)\n",
    "df.production_company = np.where(df.production_company.isin(prod_exclude), np.nan, df.production_company)\n",
    "\n",
    "# map back to list\n",
    "df.actors_lst = df.actors_lst.map(list)\n",
    "df.director_lst = df.director_lst.map(list)\n",
    "df.country_lst = df.country_lst.map(list)\n",
    "df.desc_top5 = df.desc_top5.map(list)\n",
    "df.tag_top5 = df.tag_top5.map(list)\n",
    "df.tags_rel = df.tags_rel.map(list)\n",
    "df.text_top5 = df.text_top5.map(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Get Top 3 actors from each movie based on appearances in all movies__            \n",
    "If small movie where actors only appeared a few times, will still get those actors.   \n",
    "For big movies, drop the background characters and focus on the big names    \n",
    "    \n",
    "3 hours to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_actors(row, i):\n",
    "    if len(row) >= i:\n",
    "        return row[-i][0]\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge in number of movies each actor has been in based on actors_counts dataframe\n",
    "\n",
    "#actors_explode = cat_list_expand(df, 'actors_lst')\n",
    "\n",
    "#actors_movie_counts = actors_explode.actors_lst.apply(lambda row: actors_counts[actors_counts.actors == row].cnt.values[0]\n",
    "#                                            ).to_frame()\n",
    "#actors_movie_counts.columns = ['movie_cnt']\n",
    "#actors_explode = pd.merge(actors_explode, actors_movie_counts, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for each movie, sort by the number of appearances of each actor and keep the top 3 actors  \n",
    "#actors_explode = actors_explode.sort_values(['movieId', 'movie_cnt'], ascending = False)\n",
    "#actors_explode['actor_rank'] = 1\n",
    "#actors_explode['actor_rank'] = actors_explode.groupby('movieId').actor_rank.cumsum()\n",
    "#actors_explode = actors_explode[actors_explode.actor_rank <= 3]\n",
    "\n",
    "# get list of unique actors for each movie \n",
    "#actors_explode = actors_explode.groupby('movieId').actors_lst.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe so compatible with parquet save\n",
    "#actors_explode = actors_explode.to_frame()\n",
    "#actors_explode.columns = ['actors_top3']\n",
    "#actors_explode = actors_explode.actors_top3.map(list).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actors_explode.to_parquet('processed_files/top3_actors.parq', engine = 'fastparquet', compression = 'GZIP')\n",
    "actors_explode = pd.read_parquet('processed_files/top3_actors.parq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# merge top3 actors list with main movie dataframe\n",
    "# OUTER merge so keep movies without any relevant actors \n",
    "actors_explode = actors_explode.actors_top3.map(list).to_frame()\n",
    "df = pd.merge(df, actors_explode, left_on = 'movieId', right_index = True, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when missing after merge, replace to empty list rather than null\n",
    "df.actors_top3 = df.actors_top3.apply(lambda d: d if isinstance(d, list) else [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check again for actors in 1 movie__   \n",
    "Now that have excluded some actors, once again look if any are in only 1 movie and thus not helpful for comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# expand actor list\n",
    "actors_expanded = cat_list_expand(df, 'actors_top3')\n",
    "\n",
    "# count number of movies for each actor and prep for plotting \n",
    "actors_expanded = actors_expanded.groupby('actors_top3').actors_top3.count().to_frame()\n",
    "actors_expanded.columns = ['cnt']\n",
    "actors_expanded = actors_expanded.sort_values(['cnt'])\n",
    "actors_expanded['actors'] = actors_expanded.index\n",
    "actors_exclude_top3 = actors_expanded[actors_expanded.cnt == 1].actors.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# exclude if only in 1 movie (in this processed dataset)\n",
    "df.actors_top3 = df.actors_top3.map(set) - set(actors_exclude_top3)\n",
    "\n",
    "# map back to list\n",
    "df.actors_top3 = df.actors_top3.map(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dummies + Save\n",
    "1. Dummies\n",
    "2. Save parquet\n",
    "3. Sparse + save npz   \n",
    "   \n",
    "Multiple versions with different combinations of metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Cannot run multiple versions in the same session, too much memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummies_save(df_dum, var_lst, filename):\n",
    "    \n",
    "    # create dummies\n",
    "    for var in var_lst:\n",
    "\n",
    "        prefix = var.split('_')[0]\n",
    "\n",
    "        # dummy variables\n",
    "        dummies = pd.get_dummies(df_dum[var].apply(pd.Series).stack(), prefix = prefix).sum(level = 0)\n",
    "\n",
    "        # merge back in\n",
    "        # OUTER merge so that keep if don't have any entries in the dummies (empty list)\n",
    "        df_dum = pd.merge(df_dum, dummies, left_index = True, right_index = True, how = 'outer')\n",
    "                \n",
    "            \n",
    "    # drop unnecessary columns\n",
    "    df_dum = df_dum.drop(columns = ['genre', 'genres', 'director', 'actors', 'country', 'genres_all', 'director_lst',\n",
    "                                    'actors_lst', 'country_lst', 'actors_top3', 'desc_top5', 'description',\n",
    "                                    'decade', 'production_company', 'tag_cleaned', 'tag_top5', 'tags_rel', 'text', 'text_top5'])\n",
    "        \n",
    "    # find dummy columns and replace NaN to 0 \n",
    "    dum_cols = [i for i in df_dum.columns if not i in ['movieId', 'title_eng', 'year']]    \n",
    "    for i in dum_cols:\n",
    "        df_dum[i] = np.where(df_dum[i].isnull(), 0, df_dum[i])\n",
    "    \n",
    "    # save to parquet\n",
    "    df_dum.to_parquet(filename + '.parq', engine = 'fastparquet', compression = 'GZIP')\n",
    "\n",
    "    # convert to sparse and save\n",
    "    cols = df_dum.columns # record original column names\n",
    "    cols = list(cols[3:])\n",
    "    movieIds = df_dum.movieId.to_list() # record movieIds corresponding to each row\n",
    "    df_dum = df_dum.drop(columns = ['movieId', 'title_eng', 'year'])\n",
    "    df_dum = scipy.sparse.csc_matrix(df_dum)\n",
    "\n",
    "    scipy.sparse.save_npz(filename + \"_sparse.npz\", df_dum)\n",
    "    \n",
    "    # save row and column names with sparse data\n",
    "    if filename == 'processed_files/processed_df':\n",
    "        sparse_filename = 'processed_files/sparse_metadata'\n",
    "    else:\n",
    "        sparse_filename = 'processed_files/sparse_metadata' + filename.split('_df')[1] \n",
    "\n",
    "    with open(sparse_filename, \"wb\") as f:\n",
    "        pickle.dump(cols, f)\n",
    "        pickle.dump(movieIds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummies_save(df, ['genres_all', 'actors_top3', 'director_lst'], 'processed_files/processed_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummies_save(df, ['desc_top5'], 'processed_files/processed_df_desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummies_save(df, ['genres_all', 'desc_top5'], 'processed_files/processed_df_desc_genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get additional metadata but add onto genre, actor, director set. So load in, add back columns so works in function\n",
    "#df_processed = pd.read_parquet('processed_files/processed_df.parq')\n",
    "#df_processed = pd.merge(df_processed, df[['movieId', 'genre', 'genres', 'director', 'actors', 'country', 'genres_all', \n",
    "#                                          'director_lst', 'actors_lst', 'country_lst', 'actors_top3', 'desc_top5',\n",
    "#                                          'description', 'decade', 'production_company', 'tag_cleaned', 'tag_top5', \n",
    "#                                         'tags_rel', 'text', 'text_top5']], on = 'movieId')\n",
    "#del df\n",
    "#dummies_save(df_processed, ['country_lst', 'decade', 'production_company'], 'processed_files/processed_df_all_meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get additional metadata but add onto genre, actor, director set. So load in, add back columns so works in function\n",
    "#df_processed = pd.read_parquet('processed_files/processed_df.parq')\n",
    "#df_processed = pd.merge(df_processed, df[['movieId', 'genre', 'genres', 'director', 'actors', 'country', 'genres_all', \n",
    "#                                          'director_lst', 'actors_lst', 'country_lst', 'actors_top3', 'desc_top5',\n",
    "#                                          'description', 'decade', 'production_company', 'tag_cleaned', 'tag_top5', \n",
    "#                                         'tags_rel', 'text', 'text_top5']], on = 'movieId')\n",
    "#del df\n",
    "#dummies_save(df_processed, ['tags_rel'], 'processed_files/processed_df_baseline_tags_rel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dummies_save(df, ['tag_top5'], 'processed_files/processed_df_tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummies_save(df, ['tags_rel'], 'processed_files/processed_df_tags_rel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dummies_save(df, ['text_top5'], 'processed_files/processed_df_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save movie Ids of movies with tags and movies without tags\n",
    "#with open('processed_files/movieIds_tags', \"wb\") as f:\n",
    "#    pickle.dump(df[df.tag_cleaned != ''].movieId.unique(), f)\n",
    "#with open('processed_files/movieIds_notags', \"wb\") as f:\n",
    "#    pickle.dump(df[df.tag_cleaned == ''].movieId.unique(), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
