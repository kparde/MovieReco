{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare movie data for recommendations display \n",
    "- Merge movie data with all meta-data for display + filtering\n",
    "- Calculate weighted average of ratings\n",
    "    - Primary sort key for non user filter recommendations\n",
    "    - Secondary sort key for same cosine similarity in user-item and item-item recommendations\n",
    "- Downcased versions of variables that user may input (actor, diretor for filters and title for item-item)\n",
    "- Create new variables: round average ratings to 1 decimal, separated delimited language list, length of tags lists\n",
    "- Order and rename columns for display - include non-display columns that are needed for setup at the end\n",
    "- Save as parquet: recommendation_display.parq\n",
    "            \n",
    "ALSO: limit ratings data to users that have collaborative filtering recommendations for use in app: ratings_sample_useradd_collab.parq\n",
    "- Could only train collab filtering on 5000 users due to processing limitations\n",
    "- Application also is slower if have larger ratings dataset. And having fewer users doesn't change usability or ability to prove concept in app \n",
    "- Do this limit here because still want users to be able to enter new profiles and access them to get personalized recommendations with content-based approach between retraining where we would generate collab recs for them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as datetime\n",
    "import operator\n",
    "import fastparquet\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    ### processed data from recommendation data exploration.ipynb\n",
    "    # movie attributes\n",
    "    df = pd.read_parquet('processed_files/movies_processed.parq')\n",
    "    df = df.drop(columns = ['director', 'actors', 'country', 'description', 'text',\n",
    "                            'text_top5', 'desc_top5', 'tag_cleaned', 'tags_rel', 'tag_top5'])\n",
    "    # strip year out of title. Match on ( followed by number. () sometimes valid part of title\n",
    "    df['title'] = df.title_eng.apply(lambda row: re.split('\\\\([0-9]', row)[0].strip())    \n",
    "    \n",
    "    # number of and average ratings by movie\n",
    "    movie_ratings = pd.read_parquet('processed_files/movies_ratings.parq')\n",
    "\n",
    "    ### data to get additional attributes for display\n",
    "    links = pd.read_csv('data/ml-25m/links.csv')\n",
    "    imdb_movies = pd.read_csv('data/imdb/IMDb movies.csv')\n",
    "\n",
    "    return df, movie_ratings, links, imdb_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Average Ratings\n",
    "- \\# of ratings * average rating    \n",
    "    - If just use average rating, many movies only reviewed once or twice. Want highly rated, frequently watched movies   \n",
    "    - This does weight poorly rated, but frequently watched movies higher place than well rated, infrequently movies\n",
    "- Uses:\n",
    "    - Primary sort key for non user filter recommendations\n",
    "    - Secondary sort key for same cosine similarity in user-item and item-item recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def weighted_avg(movie_ratings, df):\n",
    "    # calculate weighted average\n",
    "    movie_ratings['weighted_avg'] = movie_ratings.avg * movie_ratings.cnt\n",
    "\n",
    "    # merge with df with movie attributes\n",
    "    # LEFT merge so keep movies with no ratings (weighted avg = 0) -- can still recommend if fit specific filters\n",
    "    df = pd.merge(df, movie_ratings[['movieId', 'weighted_avg', 'cnt', 'avg']], on = 'movieId', how = 'left')\n",
    "\n",
    "    # replace nulls to 0 \n",
    "    # NOT avg: should display missing if missing\n",
    "    for var in ['weighted_avg', 'cnt']:\n",
    "        df[var] = np.where(df[var].isnull(), 0, df[var])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge in additional IMDB attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_merge(imdb_movies, links, df):\n",
    "    # standardize IMDB IDs\n",
    "    imdb_movies['imdbId'] = imdb_movies.imdb_title_id.str.split('tt').str[1]\n",
    "    imdb_movies.imdbId = pd.to_numeric(imdb_movies.imdbId)\n",
    "    \n",
    "    x = len(df)\n",
    "    # merge links to identify IMDB movies\n",
    "    df = pd.merge(df, links[['movieId', 'imdbId']], on = 'movieId')\n",
    "    # merge specific IMDB attributes\n",
    "    df = pd.merge(df, imdb_movies[['imdbId', 'description', 'language', 'duration', 'production_company']])\n",
    "    assert x == len(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downcase User Input Variables so can match user input non case-sensitive  \n",
    "Keep non-downcased version for displaying   \n",
    "Filters:\n",
    "- Actors\n",
    "- Directors     \n",
    "    \n",
    "Item-Item input: \n",
    "- Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcasing(df):\n",
    "    df['actors_downcased'] = df.actors_lst.apply(lambda row: [i.lower() for i in row])\n",
    "    df['directors_downcased'] = df.director_lst.apply(lambda row: [i.lower() for i in row])\n",
    "    df['title_downcased'] = df.title.apply(lambda row: row.lower())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new variables\n",
    "- Round average rating to 1 decimal\n",
    "- Language list delimited separate\n",
    "- Number of tags\n",
    "    - For item-item recommendations, need to know if user selected movie has any tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_vars(df):\n",
    "    # round ratings for display (1 decimal)\n",
    "    df.loc[:,'avg'] = df.loc[:,'avg'].round(1)\n",
    "    df['avg'] = df['avg'].apply(str)\n",
    "    \n",
    "    # language to list\n",
    "    for var in ['language']:\n",
    "        df[var + '_lst'] = df[var].str.split(', ')\n",
    "        df[var + '_lst'] = df[var + '_lst'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "        \n",
    "    # number of tags\n",
    "    df.tag = df.tag.apply(lambda row: [i for i in row if i != ''])\n",
    "    df['tag_num'] = df.tag.apply(lambda row: len(row))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up DataFrame for Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataframe(df):\n",
    "    \n",
    "    df_display = df.copy()\n",
    "    \n",
    "    # drop columns\n",
    "    df_display = df_display.drop(columns = ['imdbId', 'language'])\n",
    "\n",
    "    # rename + reorder columns\n",
    "    df_display.columns = ['movieId', 'title_year', 'Year', 'Production Company', 'decade', 'Genres', 'Director(s)',\n",
    "                          'Actors', 'Filming Countries', 'Tags', 'Title', 'weighted_avg', 'Number of Ratings', 'Average Rating',\n",
    "                          'Description', 'Duration (Minutes)', 'actors_downcased', 'directors_downcased', 'title_downcased',\n",
    "                           'Language(s)', 'tags_num']\n",
    "                        \n",
    "    \n",
    "    df_display = df_display[['Title', 'Year', 'Description','Duration (Minutes)', 'Genres', 'Actors', 'Director(s)', \n",
    "                             'Production Company', 'Filming Countries', 'Language(s)', 'Tags',\n",
    "                             'Number of Ratings', 'Average Rating', 'weighted_avg', 'actors_downcased', 'directors_downcased',\n",
    "                            'title_downcased', 'movieId', 'title_year', 'decade', 'tags_num']]\n",
    "    \n",
    "\n",
    "    return df_display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Limit Ratings data to Collab Filtering Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_ratings():\n",
    "    \n",
    "    # version of ratings that has manually entered user profiles added on \n",
    "    ratings = pd.read_parquet('processed_files/ratings_sample_useradd.parq')\n",
    "    ratings = ratings.reset_index(drop = True)\n",
    "\n",
    "    # precomputed collaborative filtering predictions\n",
    "    collab_predictions = pd.read_parquet('processed_files/Predictions_5000/KNN_predictions_df.parq')\n",
    "    # rename columns to be consistent \n",
    "    collab_predictions = collab_predictions.rename(columns = {'est':'prediction', 'uid':'userId', 'iid':'movieId'})\n",
    "    collab_predictions = collab_predictions.drop(columns = ['r_ui', 'details.actual_k', 'details.was_impossible'])\n",
    "\n",
    "    # drop users that do not have collaborative filtering predictions\n",
    "    ratings = ratings[ratings.userId.isin(collab_predictions.userId.unique())]\n",
    "\n",
    "    # save as parquet\n",
    "    ratings.to_parquet('processed_files/ratings_sample_useradd_collab.parq', engine = 'fastparquet', compression = 'GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # load data\n",
    "    df, movie_ratings, links, imdb_movies = load_data()\n",
    "    \n",
    "    # calculate weighted average for sorting\n",
    "    df = weighted_avg(movie_ratings, df)\n",
    "    \n",
    "    # merge in IMDB metadata and tags \n",
    "    df = imdb_merge(imdb_movies, links, df)\n",
    "        \n",
    "    # downcase user input variables so match user input non case-sensitive (keep regular casing for display) \n",
    "    df = downcasing(df)\n",
    "    \n",
    "    # new vars\n",
    "    df = new_vars(df)\n",
    "\n",
    "    # format df for display\n",
    "    df_display = display_dataframe(df)\n",
    "    \n",
    "    # save as parquet\n",
    "    df_display.to_parquet('processed_files/recommendation_display.parq', engine = 'fastparquet', compression = 'GZIP')\n",
    "    \n",
    "    # limit ratings to collaborative filtering user and save \n",
    "    limit_ratings()\n",
    "    \n",
    "    return df_display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
