{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Model\n",
    "\n",
    "## Model Comparison:\n",
    "\n",
    "1) All surprise library models are compared using crossfold validation =5 and # of users =1000. The model comparison code is commented and the comparison rmse is saved in test_rmse_score_comparison.parq. \n",
    "\n",
    "2) SVDpp() and KNNBaseline evaluation score for 1000 users were compared. The latter model was selected. The KNNBaseline performed overall better in terms of evaluations and execution time.\n",
    "\n",
    "3) KNNBaseline \n",
    "- With MSD similarity    \n",
    "- With pearson_baseline similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Selected: KNNBaseline with pearson_baseline()\n",
    "\n",
    "\n",
    "### Steps to retrain the model\n",
    "\n",
    "1. Build train dataset: Takes the processed_df.parq, ratings_sample_useradd.parq, movies_ratings.parq from the processed_files folder.\n",
    "    - filter movies with at least 50 ratings \n",
    "    - creates users_ratings data frame with only 'userId','movieId','rating' columns    \n",
    "    - Build reader: takes min and max rating from the users_ratings dataframe\n",
    "    - Build dataset using load_from_df(): this takes two arguments\n",
    "        1. users_ratings: 'userId','movieId','rating' columns\n",
    "        2. reader: created using Reader()\n",
    "        - Build train dataset using build_full_trainset(): the output is then used to fit the model.    \n",
    "    \n",
    "2. Build test set using build_anti_testset()\n",
    "    - This function creats all the user-item combinationsnot present in the actuat dataset(trainset) and returns a list as uid, iid, r_ui, here r_ui the global mean of all ratings as here user is known, item is known onlythe r_ui is known.\n",
    "3. Predictions: an estimated rating is predicted for the user-item combination in the test set.\n",
    "     - Prediction file to be used by Streamlit/EvaluationFunction.ipynb:\n",
    "     - After the predictions, the code saves the prediction dataframe to the Predictions/KNN_predictions_df.parq file which can be directly used by streamlit app or EvaluationFunction.ipynb.\n",
    "4. To get the recommendation here for a single user:\n",
    "    -  create user_profile: filter all the user-item ratings based on the user_id from the predictions sorted by highly rated movies\n",
    "    - give userId or a randon user is selected from the users_ratings dataframe\n",
    "    - top_n =10 default returns the top 10 recommendation users_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as datetime\n",
    "import operator\n",
    "import scipy.spatial.distance as distance\n",
    "from sklearn import metrics \n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import fastparquet\n",
    "from scipy.sparse import csr_matrix\n",
    "from surprise import SVD,Dataset,Reader\n",
    "from collections import defaultdict\n",
    "from surprise.model_selection import cross_validate,KFold\n",
    "from surprise import SVDpp, SlopeOne, NMF, NormalPredictor, KNNBaseline, KNNBasic, KNNWithMeans, KNNWithZScore, BaselineOnly, CoClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('processed_files/processed_df.parq')\n",
    "ratings = pd.read_parquet('processed_files/ratings_sample_useradd.parq')\n",
    "ratings = ratings.reset_index()\n",
    "movies_raitings = pd.read_parquet('processed_files/movies_ratings.parq')\n",
    "movies_raitings = movies_raitings.rename(columns={\"avg\": \"Average_Ratings\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_files/sparse_metadata', \"rb\") as f:\n",
    "    cols = pickle.load(f)\n",
    "    movieIds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering movies rated at least by 50 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of movies at least rated by 50 users: 11840\n"
     ]
    }
   ],
   "source": [
    "filtered_movies = movies_raitings[movies_raitings['cnt']>50].movieId.values\n",
    "moviesfilters = ratings[ratings.movieId.isin(filtered_movies)]\n",
    "\n",
    "print(f' Number of movies at least rated by 50 users: {len(filtered_movies)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating users list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of users who rated at least 20 movies: 40634\n"
     ]
    }
   ],
   "source": [
    "users_list = ratings.groupby('userId')['userId'].count().reset_index(name=\"rating_count\")\n",
    "print(f' Number of users who rated at least 20 movies: {len(users_list)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model trained using n_users ratings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ratings for 1000 is 150062\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "n_users = 1000\n",
    "users_list = set(users_list.userId.unique())\n",
    "random_users = random.sample(users_list, n_users)\n",
    "users_ratings = moviesfilters[moviesfilters.userId.isin(random_users)]\n",
    "print(f' ratings for {n_users} is {len(users_ratings)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering movie data for getting movie metdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_filters = np.unique(users_ratings.movieId.values)\n",
    "movie_rating = movies_raitings[movies_raitings.movieId.isin(movies_filters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ratings = users_ratings[['userId','movieId','rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting the min max rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rat = users_ratings.rating.min()\n",
    "max_rat = users_ratings.rating.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# specify the range of rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(min_rat,max_rat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading users_ratings using load_from_df for model comparison: \n",
    "The columns must correspond to user id, item id and ratings (in that order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.load_from_df(users_ratings, reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models compared \n",
    "\n",
    "Models are compared using 1000 users ratings data\n",
    "\n",
    "- SVD()\n",
    "\n",
    "- SVDpp()\n",
    "\n",
    "- SlopeOne() \n",
    "\n",
    "- NMF()\n",
    "\n",
    "- NormalPredictor()\n",
    "\n",
    "- KNNBaseline()\n",
    "\n",
    "- KNNBasic()\n",
    "\n",
    "- KNNWithMeans()\n",
    "\n",
    "- KNNWithZScore()\n",
    "\n",
    "- BaselineOnly()\n",
    "\n",
    "- CoClustering()\n",
    "\n",
    "The below block has been executed once, I have commented it now. The results are stored and can be read from the parq file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "results_test_df = []\n",
    "# Iterate over all surprise algorithms\n",
    "\n",
    "for algorithm in [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]:\n",
    "    # Perform cross validation cv =5\n",
    "    results = cross_validate(algorithm, data, measures=['RMSE'], cv=5, verbose=False)\n",
    "    \n",
    "    # Get results & append into results_test_df\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "    results_test_df.append(tmp)\n",
    "    \n",
    "pd.DataFrame(results_test_df).set_index('Algorithm').sort_values('test_rmse')  \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_test_score contains the comparison statistics of the above models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test_score = pd.DataFrame(results_test_df).set_index('Algorithm').sort_values('test_rmse')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the comparison results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test_score.to_parquet('test_rmse_score_comparison.parq', engine = 'fastparquet', compression = 'GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the top 5 models with least rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_rmse</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>test_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorithm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVDpp</th>\n",
       "      <td>0.870838</td>\n",
       "      <td>750.548304</td>\n",
       "      <td>8.109057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNBaseline</th>\n",
       "      <td>0.878081</td>\n",
       "      <td>2.111808</td>\n",
       "      <td>11.520009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaselineOnly</th>\n",
       "      <td>0.878819</td>\n",
       "      <td>0.696858</td>\n",
       "      <td>0.565961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD</th>\n",
       "      <td>0.884845</td>\n",
       "      <td>6.801081</td>\n",
       "      <td>0.555114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNWithZScore</th>\n",
       "      <td>0.893763</td>\n",
       "      <td>1.255122</td>\n",
       "      <td>7.649508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               test_rmse    fit_time  test_time\n",
       "Algorithm                                      \n",
       "SVDpp           0.870838  750.548304   8.109057\n",
       "KNNBaseline     0.878081    2.111808  11.520009\n",
       "BaselineOnly    0.878819    0.696858   0.565961\n",
       "SVD             0.884845    6.801081   0.555114\n",
       "KNNWithZScore   0.893763    1.255122   7.649508"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_score = pd.read_parquet('processed_files/test_rmse_score_comparison.parq')\n",
    "df_test_score[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the similarity option\n",
    "\n",
    "KNNBaseline \n",
    "        \n",
    "- With MSD similarity\n",
    "        \n",
    "- With pearson_baseline similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "item_based = {'name': 'cosine',\n",
    "               'user_based': False  # compute  similarities between items\n",
    "               }\n",
    "\n",
    "user_based = {'name': 'pearson_baseline',\n",
    "               'shrinkage': 0  # no shrinkage\n",
    "               }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_rmse</th>\n",
       "      <th>test_mae</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>test_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorithm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNNBaseline</th>\n",
       "      <td>0.862782</td>\n",
       "      <td>0.656062</td>\n",
       "      <td>1.125165</td>\n",
       "      <td>3.005531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNBaseline</th>\n",
       "      <td>0.872462</td>\n",
       "      <td>0.667737</td>\n",
       "      <td>12.736901</td>\n",
       "      <td>9.376762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             test_rmse  test_mae   fit_time  test_time\n",
       "Algorithm                                             \n",
       "KNNBaseline   0.862782  0.656062   1.125165   3.005531\n",
       "KNNBaseline   0.872462  0.667737  12.736901   9.376762"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_comp = []\n",
    "\n",
    "for algorithm in [KNNBaseline(sim_options=user_based), KNNBaseline(sim_options=item_based)]:\n",
    "    # Perform cross validation cv =5\n",
    "    results = cross_validate(algorithm, data, measures=['RMSE','MAE'], cv=5, verbose=False)\n",
    "    \n",
    "    # Get results & append into results_test_df\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "    results_comp.append(tmp)\n",
    "    \n",
    "pd.DataFrame(results_comp).set_index('Algorithm').sort_values('test_rmse')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Retrain\n",
    "\n",
    "\n",
    "### Final Model KNNBaseline\n",
    "\n",
    "- User-User \n",
    "- using pearson_baseline\n",
    "\n",
    "### The model code has beed commented. To retrain the model uncomment the code and execute the below. Make sure the data loading steps and the Model selection code is executed before moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "user_based = {'name': 'pearson_baseline',\n",
    "               'shrinkage': 0  # no shrinkage\n",
    "               }\n",
    "\n",
    "KNN = KNNBaseline(sim_options=user_based)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a train set using the complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainset = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit the trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNBaseline at 0x7fe4a2a0e310>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KNN.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'processed_files/KNNBaseline_model_pearson_baseline.pkl'\n",
    "#pickle.dump(KNN, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the model from the dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'processed_files/KNNBaseline_model_pearson_baseline.pkl'\n",
    "#KNNBas = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation:\n",
    "\n",
    "\n",
    "1) testset : Create user Item combination that is not available in the train set.\n",
    "\n",
    "2) predictions: predict ratings for the user-item in the test set\n",
    "\n",
    "3) Save all the predictions data into predicted_ratings\n",
    "\n",
    "All the predictions are saved into the predictions_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset = trainset.build_anti_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = KNNBas.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_ratings = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_ratings.to_parquet('Predictions/KNN_predictions_df.parq', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation starts here \n",
    "\n",
    "- If model not retrain, use the last train prediction file to get recommendations\n",
    "\n",
    "- the below code uses the latest generated prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings = pd.read_parquet('Predictions/KNN_predictions_df.parq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>3.055887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>3.112601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>7</td>\n",
       "      <td>3.104419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75</td>\n",
       "      <td>11</td>\n",
       "      <td>3.766334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75</td>\n",
       "      <td>21</td>\n",
       "      <td>3.148158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57627410</th>\n",
       "      <td>162514</td>\n",
       "      <td>30996</td>\n",
       "      <td>3.448362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57627411</th>\n",
       "      <td>162514</td>\n",
       "      <td>47330</td>\n",
       "      <td>3.448362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57627412</th>\n",
       "      <td>162514</td>\n",
       "      <td>160684</td>\n",
       "      <td>3.340844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57627413</th>\n",
       "      <td>162514</td>\n",
       "      <td>65588</td>\n",
       "      <td>3.469857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57627414</th>\n",
       "      <td>162514</td>\n",
       "      <td>7457</td>\n",
       "      <td>3.355044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57627415 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          userId  movieId  prediction\n",
       "0             75        2    3.055887\n",
       "1             75        3    3.112601\n",
       "2             75        7    3.104419\n",
       "3             75       11    3.766334\n",
       "4             75       21    3.148158\n",
       "...          ...      ...         ...\n",
       "57627410  162514    30996    3.448362\n",
       "57627411  162514    47330    3.448362\n",
       "57627412  162514   160684    3.340844\n",
       "57627413  162514    65588    3.469857\n",
       "57627414  162514     7457    3.355044\n",
       "\n",
       "[57627415 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ratings[['uid','iid','est']].rename(columns = {'est':'prediction', 'uid':'userId', 'iid':'movieId'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaborative_filtering_model(userId,movie_rating,predicted_ratings,top_n):\n",
    "    \n",
    "    \"\"\"\n",
    "    This functions recommends top_n movies to the end user\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    single_user = predicted_ratings[predicted_ratings['uid']==userId]\n",
    "    top_nmovies = single_user.sort_values(by = ['est'] , ascending = False)[:top_n]['iid']\n",
    "    \n",
    "    recommendations = pd.merge(top_nmovies,movie_rating, how='left', left_on='iid',right_on='movieId')\n",
    "    \n",
    "    \n",
    "    recommendations = recommendations[['movieId', 'title_eng', 'Average_Ratings', 'cnt']]\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get top_n recommendation for a user\n",
    "\n",
    "1) Give userId\n",
    "\n",
    "2) top_n: # of recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_list = set(users_ratings.userId.values)\n",
    "random_userId = random.sample(user_id_list, 1)\n",
    "top_n = 10\n",
    "recommendations = collaborative_filtering_model(random_userId[0],movie_rating,predicted_ratings,top_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
